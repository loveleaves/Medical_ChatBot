### use # to comment out the configure item

################ Status ################
mode=test
# string: train/test

################ Datasets(Input/Output) ################
datasets_fold=data
train_file=train.txt
dev_file=dev.txt
test_file=test.txt

delimiter=b
# string: (t: "\t";"table")|(b: "backspace";" ")|(other, e.g., '|||', ...)

use_bert=True

checkpoints_dir=checkpoints
checkpoint_name=model
vocabs_dir=data/vocabs
label2id_file=label2id
log_dir=data/logs

model_save_dir=engines/saved_model

################ Labeling Scheme ################
label_scheme=BIO
# string: BIO/BIESO

label_level=2
# int, 1:BIO/BIESO; 2:BIO/BIESO + suffix
# max to 2

hyphen=_
# string: -|_, for connecting the prefix and suffix: `B_PER', `I_LOC'

suffix=[disease,crowd,symptom,body,treatment,time,drug,feature,physiology,test,department]
# unnecessary if label_level=1

measuring_metrics=[precision,recall,f1,accuracy]
# string: accuracy|precision|recall|f1
# f1 is compulsory
measuring_metrics_level=word
# string: word/char

################ Model Configuration ################
embedding_dim=300
# int, must be consistent with `token_emb_dir' file

hidden_dim=150

max_sequence_length=512
# int, cautions! set as a LARGE number as possible,
# this will be kept during training and inferring, text having length larger than this will be truncated.

CUDA_VISIBLE_DEVICES=0
# coincides with tf.CUDA_VISIBLE_DEVICES, muti: 0,1,2

seed=42

################ Training Settings ###
epoch=20
batch_size=2

dropout=0.3
learning_rate=0.0005

optimizer=Adam
# string: SGD/Adagrad/AdaDelta/RMSprop/Adam

max_to_keep=5
# int: 0(all)/1/2/...
print_per_batch=1

is_early_stop=True
patient=3
# unnecessary if is_early_stop=False

bert_pretrain_path=bert-base-chinese
